# -*- coding: utf-8 -*-
"""Copy of storygen1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F1c5o0I9wS8Qc860_8h1h_vcuvjqptvN

# **Short StoryGen**

**Creating dataset**
"""

import csv
import random

# Some pirate-themed words and phrases
pirate_words = [
    "ship", "treasure", "captain", "crew", "ocean", "skull", "rum", "parrot",
    "cannon", "island", "pirate", "sword", "map", "blackbeard", "galleon",
    "plunder", "booty", "flag", "anchor", "sea", "storm", "voyage", "deck",
    "jolly roger", "buccaneer", "harbor", "loot", "compass", "corsair", "mast"
]

sentence_templates = [
    "The {noun} was lost during a fierce {event} at sea.",
    "Captain {name} and his {crew} sailed across the {noun}.",
    "A {noun} full of {loot} was hidden on a secret {place}.",
    "The {crew} gathered around the {noun} to plan the next raid.",
    "With a {noun} in hand, the pirate shouted orders to the {crew}.",
    "The {noun} swayed under the mighty {flag} of the {crew}.",
    "Legends say the {treasure} lies beneath the {noun}.",
    "A fierce {event} forced the crew to abandon the {noun}.",
    "The {pirate} marked the {place} on his {noun} with a red X.",
    "Rum flowed freely as the {crew} celebrated their {loot}."
]

names = [
    "Blackbeard", "Anne Bonny", "Calico Jack", "Bartholomew Roberts",
    "Mary Read", "Captain Kidd", "Henry Morgan", "Charles Vane"
]

places = ["island", "harbor", "cave", "shipwreck", "reef", "bay", "lagoon", "port"]

events = ["storm", "battle", "raid", "mutiny", "storm", "skirmish", "voyage"]

def random_word(word_list):
    return random.choice(word_list)

def generate_sentence():
    template = random.choice(sentence_templates)
    sentence = template.format(
        noun=random_word(pirate_words),
        event=random_word(events),
        name=random_word(names),
        crew=random_word(pirate_words),
        loot=random_word(pirate_words),
        place=random_word(places),
        pirate=random_word(pirate_words),
        treasure=random_word(pirate_words),
        flag=random_word(pirate_words)
    )
    return sentence

def generate_story(min_words=30):
    story = ''
    while len(story.split()) < min_words:
        sentence = generate_sentence()
        story += ' ' + sentence
    return story.strip()

def generate_title():
    # Titles are 2-4 pirate words capitalized
    return ' '.join(random.sample(pirate_words, random.randint(2, 4))).title()

def write_csv(filename='pirate_stories.csv', num_rows=50):
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=['title', 'story'])
        writer.writeheader()
        for _ in range(num_rows):
            title = generate_title()
            story = generate_story(30)
            writer.writerow({'title': title, 'story': story})

if __name__ == "__main__":
    write_csv()
    print("CSV file 'pirate_stories.csv' generated with 50 pirate-themed titles and stories.")

# Step 2: Load and format the dataset
import pandas as pd
from datasets import Dataset

# Load your short story dataset
df = pd.read_csv("/content/pirate_stories.csv")

# Combine prompt and story into one input string
df["text"] = "Prompt: " + df["title"] + "\nStory:" + df["story"]

# Drop the old columns
df = df[["text"]]

# Convert to Hugging Face Dataset object
dataset = Dataset.from_pandas(df)

# Preview
print(dataset[0]["text"])

"""**Tokenization**"""

from transformers import GPT2Tokenizer

# Load GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# GPT-2 doesn't have a padding token by default, so set it to EOS token
tokenizer.pad_token = tokenizer.eos_token

# Define a function to tokenize each example
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        padding="max_length",  # pad to max_length
        truncation=True,
        max_length=512,        # adjust max length as needed
        return_tensors="pt"
    )

# Apply tokenizer to dataset
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Check tokenized example keys and shapes
print(tokenized_dataset[0])

print(tokenized_dataset[0]["input_ids"])  # token IDs
print(tokenizer.decode(tokenized_dataset[0]["input_ids"]))  # decoded text

"""**Load GPT-2 Model and Train text**"""

from transformers import GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling

# Load GPT-2 model for causal language modeling
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Set padding token (required for GPT-2)
model.config.pad_token_id = tokenizer.eos_token_id

# Data collator for language modeling (no masking for causal LM)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./gpt2-storygen",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=100,
    save_total_limit=2,
    logging_steps=50,
    prediction_loss_only=True,
    learning_rate=5e-5,
    weight_decay=0.01,
    warmup_steps=100,

    # Disable wandb logging:
    report_to=[],
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
)

# Start training
trainer.train()

trainer.save_model("gpt2-storygen-final")
tokenizer.save_pretrained("gpt2-storygen-final")

from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline

# Loading the fine-tuned model and tokenizer
model_path = "./gpt2-storygen-final"  # or wherever you saved it
model = GPT2LMHeadModel.from_pretrained(model_path)
tokenizer = GPT2Tokenizer.from_pretrained(model_path)

# Create a text generation pipeline
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Define a prompt to generate a story from
prompt = "Prompt: A boy discovers a hidden world\nStory:\n"

# Generate story continuation
outputs = generator(
    prompt,
    max_length=200,      # total tokens generated (prompt + continuation)
    num_return_sequences=1,
    do_sample=True,      # enable sampling to get creative results
    top_p=0.6,           # nucleus sampling
    temperature=0.5      # creativity control
)

# Print the generated story
print(outputs[0]['generated_text'])